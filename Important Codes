
#Linux Nvidia GPU check
command :
watch -n 2 nvidia-smi

# NVIDIA GPU Check in windows powershell
command : 

while ($true) {
    cls
    nvidia-smi
    Start-Sleep -Seconds 2
}
------------------------------------------------------------------------------------------------------------------------
# conda create env
conda create -n env_name python=3.11
# conda remove env
conda remove --name env_name --all
# conda clean all cache/tarfiles/whl
conda clean -a
https://conda.io/projects/conda/en/latest/commands/clean.html
------------------------------------------------------------------------------------------------------------------------
# download specific file using huggingface hub
repo_id = "h94/IP-Adapter-FaceID"
filename  ="ip-adapter-faceid_sd15.bin"
from huggingface_hub import hf_hub_download
hf_hub_download(repo_id=repo_id, filename=filename,local_dir=".")
------------------------------------------------------------------------------------------------------------------------
##Line Wrapping in Collaboratory Google results
from IPython.display import HTML, display
def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)
------------------------------------------------------------------------------------------------------------------------

## forcely Killing port which activated  by python
kill -9 $(ps -A | grep python | awk '{print $1}')
------------------------------------------------------------------------------------------------------------------------ 
#install cuda using conda
conda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.0
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/
python3 -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
------------------------------------------------------------------------------------------------------------------------ 
#Get execution time on every cell
!pip install ipython-autotime
%load_ext autotime
------------------------------------------------------------------------------------------------------------------------ 
# $ nvcc --version not working
  $ sudo nano /home/$USER/.bashrc
# add this in .bashrc
   export CUDA_HOME=/usr/local/cuda
   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
   export PATH=$PATH:$CUDA_HOME/bin
#then run 
   $ source .bashrc
# then check 
   $ nvcc --version
------------------------------------------------------------------------------------------------------------------------ 
# Getting list of file size descending order
#https://kaiwern.com/posts/2020/03/16/aws-ec2-disk-space-full/
sudo du -ah . | sort -rh | head -20
------------------------------------------------------------------------------------------------------------------------ 
#shutdown linux machine using python
# it is usefull when model train is complete we want to stop machine
import subprocess
subprocess.run(["sudo", "shutdown", "-h", "now"])
------------------------------------------------------------------------------------------------------------------------ 
# transformer installation
!pip install transformers xformer accelerate einops -q
------------------------------------------------------------------------------------------------------------------------ 
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True, torch_dtype="auto")
inputs = tokenizer('''```python
def print_prime(n):
   """
   Print all primes between 1 and n
   """''', return_tensors="pt", return_attention_mask=False)

outputs = model.generate(**inputs, max_length=200)
text = tokenizer.batch_decode(outputs)[0]
print(text)

------------------------------------------------------------------------------------------------------------------------ 
#kaggle Dataset Download
import os
!pip install --upgrade --force-reinstall --no-deps kaggle
os.makedirs("/root/.kaggle",mode=600,exist_ok=True)
!cp /content/kaggle.json "/root/.kaggle"
!chmod 600 /root/.kaggle/kaggle.json
!kaggle competitions download -c feedback-prize-2021
------------------------------------------------------------------------------------------------------------------------ 

Virtal environment
py -m venv project_name #create
project_name\Scripts\activate.bat  #active
------------------------------------------------------------------------------------------------------------------------
in Seaborn plots x and y lables are comming outside of image for that we can use below code
import seaborn as sns
sns.set(style = 'whitegrid') 
------------------------------------------------------------------------------------------------------------------------
Getting requirement.txt from current environment
pip freeze > requirement.txt 
# it will Generate requirement.txt file which are currrently install in current venv
------------------------------------------------------------------------------------------------------------------------
setting option for column width maximum
pd.set_option('display.max_colwidth', -1)
df.head()
------------------------------------------------------------------------------------------------------------------------ 
#disable log in Transformer
from transformers import logging
logging.set_verbosity_error()
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
------------------------------------------------------------------------------------------------------------------------ 
# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(model, open(filename, 'wb'))

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, Y_test)
------------------------------------------------------------------------------------------------------------------------ 
import pandas as pd
df.to_pickle('123.pkl')    #to save the dataframe, df to 123.pkl
df1 = pd.read_pickle('123.pkl') #to load 123.pkl back to the dataframe df

------------------------------------------------------------------------------------------------------------------------ 
#Extracting Zip file
import zipfile
path_to_zip_file='/content/Facebook-20200618T044353Z-001.zip'
directory_to_extract_to='/content/facebook'
with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:
    zip_ref.extractall(directory_to_extract_to)
------------------------------------------------------------------------------------------------------------------------ 
## ALl type of Zip
!pip install patool
import patoolib
patoolib.extract_archive("/content/rvl-cdip.rar", outdir="/content/")
------------------------------------------------------------------------------------------------------------------------ 

# move folder iteam in other folder
import shutil
import os 
shutil.move('/content/facebook/Facebook','/content/drive/My Drive/Assignment') #move
#remove folder
shutil.rmtree('/content/drive/My Drive/Assignment/facebook') #remove 
------------------------------------------------------------------------------------------------------------------------ 
!7z e GoogleNews-vectors-negative300.bin.gz for extract gzip file
------------------------------------------------------------------------------------------------------------------------ 
#feature Importance
from sklearn.ensemble import ExtraTreesRegressor
model=ExtraTreesRegressor()
model.fit(X,y)
model.feature_importances_

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()
---------------------------------
#getting top 1000 feature index
imp_feature_index =np.argsort(r_cfl.feature_importances_)[::-1]
top_1000_bigram_feature =byte_vector_normalize[:,imp_feature_index[0:1000]]


------------------------------------------------------------------------------------------------------------------------ 
X_train, X_test, y_train, y_test = train_test_split(top_1000_bigram_feature, data_y,stratify=data_y,test_size=0.20)
# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]
X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)
------------------------------------------------------------------------------------------------------------------------ 
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
------------------------------------------------------------------------------------------------------------------------ 
from prettytable import PrettyTable
ptable = PrettyTable()
ptable.title = " Model Comparision "
ptable.field_names = ["Model",'Features','train log loss','Test log loss']
ptable.add_row(["random","Byte files","2.49","2.49"])
------------------------------------------------------------------------------------------------------------------------ 
# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
x_cfl=XGBClassifier()

prams={
    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],
     'n_estimators':[100,200,500,1000,2000],
     'max_depth':[3,5,10],
    'colsample_bytree':[0.1,0.3,0.5,1],
    'subsample':[0.1,0.3,0.5,1]
}
random_cfl1=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,) 
#rf_random =RandomizedSearchCV(rf,param_distributions=params,random_state=42,verbose=10,cv=5,scoring='mean_squared_error',n_iter = 10,return_train_score=True,n_jobs=-1)
random_cfl1.fit(X_train,y_train)
random_cfl1.best_params_

#After Random search Final Model
x_cfl=XGBClassifier(n_estimators=2000, learning_rate=0.05, colsample_bytree= 1, max_depth=10,subsample=0.3,n_jobs=-1,)
x_cfl.fit(X_train,y_train)
c_cfl=CalibratedClassifierCV(x_cfl,method='sigmoid')
c_cfl.fit(X_train,y_train)

------------------------------------------------------------------------------------------------------------------------ 
#Random Search CV
results=pd.DataFrame(rf_random.cv_results_)
results.head()

import seaborn as sns;
sns.set()
data = results.pivot('param_min_samples_split', 'param_max_depth', "mean_train_score")
ax = sns.heatmap(data, annot=True).set(title="Min Sample split ,Max depth AUC Score", xlabel='Max Depth',ylabel='Min Sample Split')


------------------------------------------------------------------------------------------------------------------------ 

results = results.sort_values(['param_max_depth'])
train_auc= results['mean_train_score']
train_auc_std= results['std_train_score']
cv_auc = results['mean_test_score']
cv_auc_std= results['std_test_score']
max_depth = results['param_max_depth']
plt.plot(max_depth,train_auc,label='Train MSE')
plt.plot(max_depth,cv_auc,label='CV MSE')
plt.scatter(max_depth,train_auc,label='Train MSE Points')
plt.scatter(max_depth,cv_auc,label="CV MSE Points")
plt.legend()
plt.xlabel("max_depth: Hyperparameter")
plt.ylabel("Neg MSE")
plt.title("Error PLots")
plt.grid()
plt.show()

------------------------------------------------------------------------------------------------------------------------ 
#hstack Example
a = np.array([[1],[2],[3]])
b = np.array([[2],[3],[4]])
np.hstack((a,b))
array([[1, 2],
       [2, 3],
       [3, 4]])
---------------
#vstack Example
a = np.array([[1], [2], [3]])
b = np.array([[2], [3], [4]])
np.vstack((a,b))
array([[1],
       [2],
       [3],
       [2],
       [3],
       [4]])
------------------------------------------------------------------------------------------------------------------------ 
#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html
X= final_dataset.iloc[:,1:] ## selection except then 0 column others like 0,1,2,3,4 columns are there it will select 1,2,3,4 column
y=final_dataset.iloc[:,0] ## it will select 0th column others like 0,1,2,3,4 columns are there it will select only 0 colum
------------------------------------------------------------------------------------------------------------------------ 
Enable Jupyter notebook Extention 
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
jupyter nbextension enable spellchecker/main #spelling checking
------------------------------------------------------------------------------------------------------------------------ 
# WSL internet Solution
sudo rm /etc/resolv.conf
sudo bash -c 'echo "nameserver 8.8.8.8" > /etc/resolv.conf'
sudo bash -c 'echo "[network]" > /etc/wsl.conf'
sudo bash -c 'echo "generateResolvConf = false" >> /etc/wsl.conf'
sudo chattr +i /etc/resolv.conf
